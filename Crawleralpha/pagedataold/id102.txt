Analysis of Algorithms | Set 3 (Asymptotic Notations) - GeeksforGeeks GeeksforGeeks A computer science portal for geeks GeeksQuiz Login Home Algorithms DS GATE Interview Corner Q&A C C++ Java Books Contribute Ask a Q About Array Bit Magic C/C++ Articles GFacts Linked List MCQ Misc Output String Tree Graph Analysis of Algorithms | Set 3 (Asymptotic Notations) We have discussed Asymptotic Analysis, and Worst, Average and Best Cases of Algorithms. The main idea of asymptotic analysis is to have a measure of efficiency of algorithms that doesn’t depend on machine specific constants, and doesn’t require algorithms to be implemented and time taken by programs to be compared. Asymptotic notations are mathematical tools to represent time complexity of algorithms for asymptotic analysis. The following 3 asymptotic notations are mostly used to represent time complexity of algorithms. 1) Notation: The theta notation bounds a functions from above and below, so it defines exact asymptotic behavior. A simple way to get Theta notation of an expression is to drop low order terms and ignore leading constants. For example, consider the following expression. 3n3 + 6n2 + 6000 = (n3) Dropping lower order terms is always fine because there will always be a n0 after which (n3) beats (n2) irrespective of the constants involved. For a given function g(n), we denote (g(n)) is following set of functions. 
((g(n)) = {f(n): there exist positive constants c1, c2 and n0 such that
                  0 <= c1*g(n) <= f(n) <= c2*g(n) for all n >= n0} The above definition means, if f(n) is theta of g(n), then the value f(n) is always between c1*g(n) and c2*g(n) for large values of n (n >= n0). The definition of theta also requires that f(n) must be non-negative for values of n greater than n0. 2) Big O Notation: The Big O notation defines an upper bound of an algorithm, it bounds a function only from above. For example, consider the case of Insertion Sort. It takes linear time in best case and quadratic time in worst case. We can safely say that the time complexity of Insertion sort is O(n^2). Note that O(n^2) also covers linear time. If we use notation to represent time complexity of Insertion sort, we have to use two statements for best and worst cases: 1. The worst case time complexity of Insertion Sort is (n^2). 2. The best case time complexity of Insertion Sort is (n). The Big O notation is useful when we only have upper bound on time complexity of an algorithm. Many times we easily find an upper bound by simply looking at the algorithm. 
O(g(n)) = { f(n): there exist positive constants c and n0 such that 
            0 <= f(n) <= cg(n) for all n >= n0}  3) Notation: Just as Big O notation provides an asymptotic upper bound on a function, notation provides an asymptotic lower bound. Notation< can be useful when we have lower bound on time complexity of an algorithm. As discussed in the previous post, the best case performance of an algorithm is generally not useful, the Omega notation is the least used notation among all three. For a given function g(n), we denote by (g(n)) the set of functions. 
 (g(n)) = {f(n): there exist positive constants c and n0 such that 
                            0 <= cg(n) <= f(n) for all n >= n0}. Let us consider the same Insertion sort example here. The time complexity of Insertion Sort can be written as (n), but it is not a very useful information about insertion sort, as we are generally interested in worst case and sometimes in average case. Exercise: Which of the following statements is/are valid? 1. Time Complexity of QuickSort is (n^2) 2. Time Complexity of QuickSort is O(n^2) 3. For any two functions f(n) and g(n), we have f(n) = (g(n)) if and only if f(n) = O(g(n)) and f(n) = (g(n)). 4. Time complexity of all computer algorithms can be written as (1) References: Lec 1 | MIT (Introduction to Algorithms) Introduction to Algorithms 3rd Edition by Clifford Stein, Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest This article is contributed by Abhay Rathi. Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.           Related Topics: A Problem in Many Binary Search Implementations Analysis of Algorithms | Set 4 (Analysis of Loops) NP-Completeness | Set 1 (Introduction) Static and Dynamic Libraries | Set 1 The Ubiquitous Binary Search | Set 1 Reservoir Sampling Analysis of Algorithms | Set 2 (Worst, Average and Best Cases) Analysis of Algorithms | Set 1 (Asymptotic Analysis) Tweet Writing code in comment? Please use ideone.com and share the link here. Marsha Donna so does this mean that big O notation corresponds to worst case time complexity, big omega notation corresponds to best case time complexity, big theta notation corresponds to average case time complexity??? Ankur Teotia i have the same doubt , someone please clarify. it seems to be the case but no where i see this explicitly mentioned. darubramha Somewhat correct. big O notation corresponds to worst case time complexity, generally it means “less than or equal to”, i.e if T(n) [time complexity] = O(n) then it means, T(n) can have maximum cn value, -> T(n) =” as explained in the previous case. But, big theta notation, tells us that the time complexity of the algorithm is asymptotically bounded by the given function, which means if T(n) = theta(n), the time complexity can never be worse than c1n or can never be better than c2n, where c1,c2 are constants. It does not mean the average case, it corresponds more towards “=”, i.e c1n <= T(n) <= c2n noob 1. Time Complexity of QuickSort is (n^2) —–> NO 2. Time Complexity of QuickSort is O(n^2) —–> YES 3. For any two functions f(n) and g(n), we have f(n) = (g(n)) if and only if f(n) = O(g(n)) and f(n) = (g(n)). —–> YES 4. Time complexity of all computer algorithms can be written as (1) —–> YES ibn Why was question 4 Yes? mog bcoz oemga always provide a lower bound on running time of algorithm.so minimum running time would be omega(1),it cant be less than omega(1)…..so we can write time complexity of all algorithms as omega(1). Sourabh For a sorting algorithm, best case can not be better than omega(n) as you have to check all n elements before saying that it is sorted, so I guess answer is NO! rohit 1 yes Utkarsh Gupta If f(n) = O(n) then it means f(n) can be less than or equal to order of n. If f(n) = ?(n) then it means f(n) is equal to the order of n. If f(n) = ?(n) then it means f(n) is greater than or equal to order of n. So if you see ?(1), it means that this is the smallest lower bound for any algorithm. Therefore for any algorithm f(n) = ?(1). But it can be even more precise that is f(n) = ?(n) or ?(n^2) etc for different different conditions. Chaithanya Kanumolu I am confused with 4th point. Since best case for insertion sort is Theta(n) right? In that case how can we write it as Theta (1)? Interview Experiences Advanced Data Structures Dynamic Programming Greedy Algorithms Backtracking Pattern Searching Divide & Conquer Mathematical Algorithms Recursion Geometric Algorithms Popular Posts All permutations of a given string Memory Layout of C Programs Understanding “extern” keyword in C Median of two sorted arrays Tree traversal without recursion and without stack! Structure Member Alignment, Padding and Data Packing Intersection point of two Linked Lists Lowest Common Ancestor in a BST. Check if a binary tree is BST or not Sorted Linked List to Balanced BST Follow @GeeksforGeeks Subscribe Recent Comments @geeksforgeeks, Some rights reserved        Contact Us! Powered by WordPress & MooTools, customized by geeksforgeeks team
